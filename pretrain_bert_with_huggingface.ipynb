{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "pretrain_bert_with_huggingface.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyNIdl7mWad0uF0syDawM8UD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JayThibs/pretrain-nlp-models/blob/main/pretrain_bert_with_huggingface.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YWgD3OL0PFLV"
      },
      "source": [
        "# Pre-Training a BERT model with Huggingface\n",
        "\n",
        "We will be following the tutorial found here: https://huggingface.co/blog/how-to-train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3fkOIQhdO3Jo",
        "outputId": "c04c81d1-965e-41f2-90c7-f5c9774ded8e"
      },
      "source": [
        "!pip install transformers --quiet"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 2.6 MB 15.5 MB/s \n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 76.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 636 kB 59.6 MB/s \n",
            "\u001b[K     |████████████████████████████████| 895 kB 66.4 MB/s \n",
            "\u001b[?25h"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-2jq1WhXVDSd",
        "outputId": "ff63c854-ff06-4741-ac88-f89aecca006d"
      },
      "source": [
        "# download dataset\n",
        "!wget -c https://cdn-datasets.huggingface.co/EsperBERTo/data/oscar.eo.txt"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-08-20 20:52:24--  https://cdn-datasets.huggingface.co/EsperBERTo/data/oscar.eo.txt\n",
            "Resolving cdn-datasets.huggingface.co (cdn-datasets.huggingface.co)... 65.9.73.8, 65.9.73.71, 65.9.73.86, ...\n",
            "Connecting to cdn-datasets.huggingface.co (cdn-datasets.huggingface.co)|65.9.73.8|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 312733741 (298M) [text/plain]\n",
            "Saving to: ‘oscar.eo.txt’\n",
            "\n",
            "oscar.eo.txt        100%[===================>] 298.25M  43.2MB/s    in 6.7s    \n",
            "\n",
            "2021-08-20 20:52:31 (44.3 MB/s) - ‘oscar.eo.txt’ saved [312733741/312733741]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "id": "gwbqNt5-PSAJ",
        "outputId": "c497ddd9-6132-4929-91a4-50e63a4a2c60"
      },
      "source": [
        "from pathlib import Path\n",
        "from tokenizers import ByteLevelBPETokenizer\n",
        "\n",
        "paths = [str(x) for x in Path('.').glob('**/*.txt')]\n",
        "\n",
        "# initialize a tokenizer\n",
        "tokenizer = ByteLevelBPETokenizer()\n",
        "\n",
        "# customize training\n",
        "tokenizer.train(files=paths, vocab_size=52000, min_frequency=2, special_tokens=[\"<s>\", \"<pad>\", \"</s>\", \"<unk>\", \"<mask>\"])\n",
        "\n",
        "# save files to disk\n",
        "tokenizers.save_model('.', 'esperberto')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-8e7ef3e45949>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# save files to disk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mtokenizers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'esperberto'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'tokenizers' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o7Ug08wzVhs6",
        "outputId": "4bbb6a4a-a298-4c90-d13d-d3a8802ac179"
      },
      "source": [
        "!mkdir EsperBERTo\n",
        "tokenizer.save_model(\"EsperBERTo\")"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['EsperBERTo/vocab.json', 'EsperBERTo/merges.txt']"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xx0gpMMsVuHV"
      },
      "source": [
        "We now have both a `vocab.json`, which is a list of the most frequent tokens ranked by frequency, and a `merges.txt` list of merges.\n",
        "\n",
        "```json\n",
        "{\n",
        "\t\"<s>\": 0,\n",
        "\t\"<pad>\": 1,\n",
        "\t\"</s>\": 2,\n",
        "\t\"<unk>\": 3,\n",
        "\t\"<mask>\": 4,\n",
        "\t\"!\": 5,\n",
        "\t\"\\\"\": 6,\n",
        "\t\"#\": 7,\n",
        "\t\"$\": 8,\n",
        "\t\"%\": 9,\n",
        "\t\"&\": 10,\n",
        "\t\"'\": 11,\n",
        "\t\"(\": 12,\n",
        "\t\")\": 13,\n",
        "\t# ...\n",
        "}\n",
        "\n",
        "# merges.txt\n",
        "l a\n",
        "Ġ k\n",
        "o n\n",
        "Ġ la\n",
        "t a\n",
        "Ġ e\n",
        "Ġ d\n",
        "Ġ p\n",
        "# ...\n",
        "```\n",
        "\n",
        "What is great is that our tokenizer is optimized for Esperanto. Compared to a generic tokenizer trained for English, more native words are represented by a single, unsplit token. Diacritics, i.e. accented characters used in Esperanto – `ĉ`, `ĝ`, `ĥ`, `ĵ`, `ŝ`, and `ŭ` – are encoded natively. We also represent sequences in a more efficient manner. Here on this corpus, the average length of encoded sequences is ~30% smaller as when using the pretrained GPT-2 tokenizer.\n",
        "\n",
        "Here’s  how you can use it in `tokenizers`, including handling the RoBERTa special tokens – of course, you’ll also be able to use it directly from `transformers`.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bq9XzrYRVojg"
      },
      "source": [
        "from tokenizers.implementations import ByteLevelBPETokenizer\n",
        "from tokenizers.processors import BertProcessing\n",
        "\n",
        "tokenizer = ByteLevelBPETokenizer(\n",
        "    \"./EsperBERTo/vocab.json\",\n",
        "    \"./EsperBERTo/merges.txt\",\n",
        ")"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cdxe9aYVWHhx",
        "outputId": "bb85ef97-5d63-4344-87c1-d2c64ece8aed"
      },
      "source": [
        "tokenizer.encode(\"Mi estas Jacques.\")"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Encoding(num_tokens=4, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kEj4th-cWPgC",
        "outputId": "84454895-16d6-48f0-8427-e2b364c9e028"
      },
      "source": [
        "tokenizer.encode(\"Mi estas Jacques.\").tokens"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Mi', 'Ġestas', 'ĠJacques', '.']"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "atTnaR4UWW64"
      },
      "source": [
        "# 3. Train a lanaguage model from scratch\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RcnX8njRWT6R"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}